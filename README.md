# Kobuki Robot-collaboration with Cogniteam - 

Showing gestures that express a wide range of "emotions" of the robot in front of people. 
The research is designed to check whether people understand the emotion the robot is trying to express by the movement it makes.

As part of the project, I worked with an animator who created the gestures and I translated them into actual data in a json file.
Any animation (gesture) that you want to implement in the robot is written according to fixed parameters in the json file.

Technologies I used during work -

Since the work was on a cogniteam robot, I had to upload the component (on which the jsin file I am writing sits) to the
company's platform through which the robot can be controlled (the name of the platform is Nimbus).
Among other things, an understanding of Docker was required of me in order to run the robot.

Gestures that the robot makes according to the parameters of the json file -

The robot:

<img width="255" alt="צילום מסך 2024-03-09 ב-16 58 39" src="https://github.com/omriamidi/Tray_Robot/assets/111886837/2c0dcfc6-0fbb-45a7-80e0-33a14dc67ee9">



Sample 1:

[gesture 2.MOV.zip](https://github.com/omriamidi/Tray_Robot/files/14542959/gesture.2.MOV.zip)


Sample 2:

[gesture 1.MOV.zip](https://github.com/omriamidi/Tray_Robot/files/14542982/gesture.1.MOV.zip)
