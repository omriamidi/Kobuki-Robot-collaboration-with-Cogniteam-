# Kobuki Robot-collaboration with Cogniteam - 

Showing gestures that express a wide range of "emotions" of the robot in front of people. 
The research is designed to check whether people understand the emotion the robot is trying to express by the movement it makes.

As part of the project, I worked with an animator who created the gestures and I translated them into actual data in a json file.

Any animation (gesture) that you want to implement in the robot is written according to fixed parameters in the json file.

Gestures that the robot makes according to the parameters of the json file -

Sample 1:

[gesture 2.MOV.zip](https://github.com/omriamidi/Tray_Robot/files/14542959/gesture.2.MOV.zip)


Sample 2:

[gesture 1.MOV.zip](https://github.com/omriamidi/Tray_Robot/files/14542982/gesture.1.MOV.zip)
